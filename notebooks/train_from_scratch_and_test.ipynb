{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IzLN5nvyN5WF"
   },
   "outputs": [],
   "source": [
    "!wget http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
    "!tar xzf dev-clean.tar.gz\n",
    "!sudo apt-get install tree -y\n",
    "!pip install soundfile\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s953V-yZ8hB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape, Concatenate,concatenate, Average\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pDe6shDcq1F"
   },
   "outputs": [],
   "source": [
    "def graph_spectrogram(wav_file):\n",
    "    rate, data = get_wav_info(wav_file)\n",
    "    nfft = 200 # Length of each window segment\n",
    "    fs = 8000 # Sampling frequencies\n",
    "    noverlap = 120 # Overlap between windows\n",
    "    nchannels = data.ndim\n",
    "    if nchannels == 1:\n",
    "        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n",
    "    elif nchannels == 2:\n",
    "        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n",
    "    return pxx\n",
    "\n",
    "# Load a wav file\n",
    "def get_wav_info(wav_file):\n",
    "    rate, data = wavfile.read(wav_file)\n",
    "    return rate, data\n",
    "\n",
    "def process_sliding_window(flac_file, processed_directory_path,filename):\n",
    "    window_size = 1600\n",
    "    step_size = 800\n",
    "    actual_idx = 0\n",
    "    amount_of_files = 0\n",
    "    new_base_directory = processed_directory_path +filename[:-5]+\"/\"\n",
    "    new_base_path = (new_base_directory+filename)[:-5]\n",
    "\n",
    "    if not os.path.exists(new_base_directory):\n",
    "        \tos.makedirs(new_base_directory)\n",
    "\n",
    "    while(len(flac_file) > actual_idx + window_size):\n",
    "        flac_file[actual_idx:actual_idx+window_size].export(new_base_path+'_'+str(amount_of_files) + '.wav',format='wav')\n",
    "        amount_of_files += 1\n",
    "        actual_idx += step_size\n",
    "    if (actual_idx + window_size - len(flac_file) > window_size/2 and len(flac_file) > window_size):\n",
    "        flac_file[-window_size:].export(new_base_path+'_'+str(amount_of_files) + '.wav',format='wav')\n",
    "        \n",
    "\n",
    "def make_processed_dataset(libri_speech_path,processed_libri_speech_path):\n",
    "    dev_path = libri_speech_path + \"dev-clean/\"\n",
    "    processed_dev_path = processed_libri_speech_path + \"dev-clean/\"\n",
    "    voice_count = 0\n",
    "    for voice in (os.listdir(dev_path)):\n",
    "        voice_path = dev_path + voice + \"/\"\n",
    "        for chapter in (os.listdir(voice_path)):\n",
    "            chapter_path = voice_path + chapter + \"/\"\n",
    "            for filename in os.listdir(chapter_path):\n",
    "                processed_directory_path = processed_dev_path + voice + \"/\" + chapter + \"/\" \n",
    "                if filename.endswith(\"flac\"):\n",
    "                    flac_file = AudioSegment.from_file(chapter_path + filename, \"flac\")\n",
    "                    process_sliding_window(flac_file,processed_directory_path,filename)\n",
    "        voice_count += 1\n",
    "        print (\"actual progress\", str (int(voice_count/len(os.listdir(dev_path))*100)), ('%'))\n",
    "        \n",
    "def process_16_dataset_main():\n",
    "    dataset_base_path = \"./\"\n",
    "    libri_speech_path = dataset_base_path + \"LibriSpeech/\"\n",
    "    processed_libri_speech_path = dataset_base_path + \"16_LibriSpeech/\"\n",
    "    make_processed_dataset(libri_speech_path, processed_libri_speech_path)\n",
    "\n",
    "\n",
    "def make_numpy_XY(libri_speech_path):\n",
    "    dev_path = libri_speech_path + \"dev-clean/\"\n",
    "    voice_count = 0\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for voice in (os.listdir(dev_path)):\n",
    "        voice_path = dev_path + voice + \"/\"\n",
    "        count = 0     \n",
    "        X_5 = []\n",
    "        for chapter in (os.listdir(voice_path)):\n",
    "            chapter_path = voice_path + chapter + \"/\"\n",
    "            for uterrance in os.listdir(chapter_path):\n",
    "              uterrance_path = chapter_path + uterrance + \"/\"\n",
    "              for filename in os.listdir(uterrance_path):\n",
    "                if filename.endswith(\"wav\"):\n",
    "                    count += 1\n",
    "                    wav_file = graph_spectrogram(uterrance_path + filename)\n",
    "                    X_5.append(wav_file)\n",
    "                if (count % 5 == 0 and count != 0):\n",
    "                    X.append(X_5)\n",
    "                    X_5 = []\n",
    "                    Y.append(voice)\n",
    "                if (count >= 100):\n",
    "                    break\n",
    "            if count>=100:\n",
    "                break\n",
    "        voice_count+=1\n",
    "        print (\"actual progress\", str (int(voice_count/len(os.listdir(dev_path))*100)), ('%'))\n",
    "    return X,Y\n",
    "\n",
    "def process_numpy_XY():\n",
    "    dataset_base_path = \"./\"\n",
    "    processed_libri_speech_path = dataset_base_path + \"16_LibriSpeech/\"\n",
    "    X,Y = make_numpy_XY(processed_libri_speech_path)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X,Y\n",
    "\n",
    "def reshape_X(X):\n",
    "    examples = X.shape[0]\n",
    "    dimension = X.shape[1]\n",
    "    time = X.shape[2]\n",
    "    freq = X.shape[3]\n",
    "    new_X = np.zeros((dimension,examples,time,freq))\n",
    "    for m in range(examples):\n",
    "        for d in range(dimension):\n",
    "            new_X[d][m] = X[m][d]\n",
    "    return new_X\n",
    "    \n",
    "    \n",
    "def load_numpy_XY():\n",
    "    X_path = './preprocessed/X_total.npy'\n",
    "    Y_path = './preprocessed/Y_total.npy'\n",
    "    X = np.load(X_path)\n",
    "    Y = np.load(Y_path)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-LS3q6Y3AxMI"
   },
   "outputs": [],
   "source": [
    "print ('processing dataset:')\n",
    "process_16_dataset_main()\n",
    "print ('----------------')\n",
    "print ('Making XY:')\n",
    "X,Y = process_numpy_XY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nafZHdeVA1Xd"
   },
   "outputs": [],
   "source": [
    "def five_average_model(input_shape,base_model):\n",
    "    model_inputs = []\n",
    "    model_results = []\n",
    "    for i in range(5):\n",
    "        X_input = Input(shape = input_shape, name = \"Input\"+ str(i))\n",
    "        model_inputs.append(X_input)\n",
    "        result = base_model(X_input)\n",
    "        model_results.append(result)\n",
    "\n",
    "    X = Average()(model_results)\n",
    "    model = Model(inputs = model_inputs,outputs = X)\n",
    "  \n",
    "    return model\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/(norm_u*norm_v)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "  \n",
    "def triplet_loss(y_true, y_pred, alpha = 0.35):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    total_lenght = 64*3\n",
    "    anchor, positive, negative = y_pred[:,0:int(total_lenght*1/3)],y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)],y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.sum(K.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "def base_model(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    X = Conv1D(196,kernel_size = 15, strides = 4)(X_input)\n",
    "    X = Activation('relu')(X)                                \n",
    "    X = Dropout(rate = 0.2)(X)                                 \n",
    "    \n",
    "    X = LSTM(units = 128, return_sequences = True)(X_input)               \n",
    "    X = Dropout(rate = 0.2)(X)                                 \n",
    "    \n",
    "    X = LSTM(units = 128, return_sequences = True)(X)                         \n",
    "    X = Dropout(rate = 0.2)(X)                                \n",
    "    \n",
    "    X = LSTM(units = 128)(X)                                 \n",
    "    X = Dropout(rate = 0.2)(X)                               \n",
    "    \n",
    "    X = Dense(64)(X)\n",
    "    \n",
    "    base_model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return base_model  \n",
    "  \n",
    "def speech_model(input_shape, average_model):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "    base_model -- model to be used to call the inputs\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    #get triplets vectors\n",
    "    input_anchor = []\n",
    "    input_positive = []\n",
    "    input_negative = []\n",
    "    for i in range(15):\n",
    "        X_input = Input(shape = input_shape, name = \"Input\"+ str(i))\n",
    "        if (i < 5):\n",
    "            input_anchor.append(X_input)\n",
    "        elif (i < 10):\n",
    "            input_positive.append(X_input)\n",
    "        elif(i < 15):\n",
    "            input_negative.append(X_input)\n",
    "        \n",
    "    vec_anchor = average_model(input_anchor)\n",
    "    vec_positive = average_model(input_positive)\n",
    "    vec_negative = average_model(input_negative)\n",
    "    \n",
    "    #Concatenate vectors vec_positive, vec_negative\n",
    "    concat_layer = concatenate([vec_anchor,vec_positive,vec_negative], axis = -1, name='concat_layer')\n",
    "    \n",
    "    model = Model(inputs = input_anchor + input_positive + input_negative, outputs = concat_layer, name = 'speech_to_vec')\n",
    "    #model = Model(inputs = input_anchor + input_positive + input_negative, outputs = [vec_anchor,vec_positive,vec_negative], name = 'speech_to_vec')\n",
    "    #model = Model(inputs = [input_anchor,input_positiv], outputs=vec_anchor)\n",
    "    \n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9j_htdOHA9wP"
   },
   "outputs": [],
   "source": [
    "d = 5\n",
    "Tx = 318\n",
    "n_freq = 101\n",
    "\n",
    "base_model = base_model(input_shape = (n_freq, Tx))\n",
    "average_model = five_average_model(input_shape = (n_freq, Tx), base_model = base_model)\n",
    "speech_model = speech_model(input_shape = (n_freq, Tx), average_model = average_model) \n",
    "speech_model.compile(loss=triplet_loss, optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzH7UFKpBGBI"
   },
   "outputs": [],
   "source": [
    "print (X.shape)\n",
    "X_train, X_test = [], []\n",
    "Y_train, Y_test = [], []\n",
    "for i in range (X.shape[0]):\n",
    "    if np.random.random() < 0.8:\n",
    "        X_train.append(X[i])\n",
    "        Y_train.append(Y[i])\n",
    "    else:\n",
    "        X_test.append(X[i])\n",
    "        Y_test.append(Y[i])\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyruENLxBH3z"
   },
   "outputs": [],
   "source": [
    "X_train = reshape_X(X_train)\n",
    "X_test = reshape_X(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uI-kOIL8BKWS"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "!mkdir preprocessed\n",
    "np.save('./preprocessed/X_train',X_train)\n",
    "np.save('./preprocessed/X_test',X_test)\n",
    "np.save('./preprocessed/Y_train',Y_train)\n",
    "np.save('./preprocessed/Y_test',Y_test)\n",
    "print (X.shape)\n",
    "print (Y.shape)\n",
    "\n",
    "np.save('./preprocessed/X_total',X)\n",
    "np.save('./preprocessed/Y_total',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7iImQc8BLtI"
   },
   "outputs": [],
   "source": [
    "X_train = np.load('./preprocessed/X_train.npy')\n",
    "X_test = np.load('./preprocessed/X_test.npy')\n",
    "Y_train = np.load('./preprocessed/Y_train.npy')\n",
    "Y_test = np.load('./preprocessed/Y_test.npy')\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1gI9W4aBM7A"
   },
   "outputs": [],
   "source": [
    "def average_emb(X_predicts,Y_predicts):\n",
    "    average_embeddings = {}\n",
    "    count_embeddings = {}\n",
    "    for i in range(len(X_predicts)):\n",
    "        if Y_predicts[i] not in average_embeddings:\n",
    "            average_embeddings[Y_predicts[i]] = X_predicts[i]\n",
    "            count_embeddings[Y_predicts[i]] = 1\n",
    "        else:\n",
    "            average_embeddings[Y_train[i]] += X_predicts[i]\n",
    "            count_embeddings[Y_predicts[i]] += 1\n",
    "\n",
    "    for key, value in average_embeddings.items():\n",
    "        average_embeddings[key] = average_embeddings[key]/count_embeddings[key]\n",
    "    return average_embeddings\n",
    "\n",
    "  \n",
    "def get_acurracy(X_predicts):\n",
    "    average_embeddings = average_emb(X_predicts,Y_train)\n",
    "    count = 0\n",
    "    bad_count = 0\n",
    "    for i in range(len(X_predicts)):\n",
    "    best_result = -1\n",
    "    best_val = 0\n",
    "    for key, value in average_embeddings.items():\n",
    "        result = cosine_similarity(X_predicts[i],value)\n",
    "        #print (key, Y_test[i],result)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_val = key\n",
    "\n",
    "    if (best_val == Y_train[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        bad_count+=1\n",
    "\n",
    "    return (count/(count+bad_count))\n",
    "\n",
    "\n",
    "def make_matrix(X_train,Y_train,average_model):\n",
    "    print('making matrix')\n",
    "    X_predicts = average_model.predict(X_train,batch_size = 16,verbose = 1)\n",
    "    accuracy = get_acurracy(X_predicts)\n",
    "    matrix = np.zeros((len(X_train[0]),len(X_train[0])))\n",
    "    for i in range(len(X_predicts)):\n",
    "        matrix[i][i] = 1.0\n",
    "        for j in range(i+1,len(X_predicts)):\n",
    "            matrix[i][j] = cosine_similarity(X_predicts[i],X_predicts[j])\n",
    "            matrix[j][i] = matrix[i][j]\n",
    "    return accuracy,matrix\n",
    "\n",
    "def make_worse_triplets(matrix,X_train,Y_train):\n",
    "    anchors = np.zeros((5,len(X_train[0]),101,318))\n",
    "    positives = np.zeros((5,len(X_train[0]),101,318))\n",
    "    negatives = np.zeros((5,len(X_train[0]),101,318))\n",
    "    for x in range(len(matrix)):\n",
    "        lowest = 1.5\n",
    "        highest = -1.5\n",
    "        anchor = X_train[:,x:x+1]\n",
    "        for y in range(len(matrix)):\n",
    "            if matrix[x][y] < lowest and (Y_train[x] == Y_train[y]):\n",
    "                lowest = matrix[x][y]\n",
    "                positive = y\n",
    "            elif matrix[x][y] > highest and Y_train[x] != Y_train[y]:\n",
    "                highest = matrix[x][y]\n",
    "                negative = y\n",
    "        anchors[:,x,:,:] = X_train[:,x,:,:]\n",
    "        positives[:,x,:,:] = X_train[:,positive,:,:]\n",
    "        negatives[:,x,:,:] = X_train[:,negative,:,:]\n",
    "        #print (x,'/',len(matrix))\n",
    "    return anchors,positives,negatives\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6HIzlmfBPTJ"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 32\n",
    "Y_dummy = np.empty((batch_size, 3))\n",
    "messages= []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_kNAUk0BQfu"
   },
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    print (\"epoch\",i)\n",
    "    accuracy,matrix = make_matrix(X_train.tolist(),Y_train,bas e_model)\n",
    "    print (\"made matrix\")\n",
    "    print(\"acurracy\",accuracy)\n",
    "    accuracies += [accuracy]\n",
    "    anchors, positives, negatives = make_worse_triplets(matrix,X_train,Y_train)\n",
    "    start_idx = 0\n",
    "    mean_loss = 0\n",
    "    while (start_idx + batch_size < len(anchors[0])):\n",
    "        batch_x = anchors[:,start_idx:start_idx + batch_size].tolist()+positives[:,start_idx:start_idx + batch_size].tolist()+negatives[:,start_idx:start_idx + batch_size].tolist()\n",
    "        message = speech_model.train_on_batch(x = batch_x, y = Y_dummy)\n",
    "        mean_loss+= message[0]\n",
    "        messages+= [message]\n",
    "        if ((start_idx/batch_size)%3 == 0):\n",
    "            print (\"start_idx:\",start_idx, \",loss:\",message)\n",
    "        start_idx+=batch_size\n",
    "    if (len(anchors[0])%batch_size != 0):\n",
    "        batch_x = anchors[:,-batch_size:].tolist()+positives[:,-batch_size:].tolist()+negatives[:,-batch_size:].tolist()\n",
    "        speech_model.train_on_batch(x = batch_x, y = Y_dummy)\n",
    "    print(\"average_loss:\",(mean_loss/(len(anchors[0])/batch_size+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMfm135_kz-H"
   },
   "outputs": [],
   "source": [
    "messages_donwload = np.array(messages)\n",
    "acc_down = np.array(accuracies)\n",
    "print (messages_donwload.shape)\n",
    "print (acc_down.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zp-VQoDqk9iB"
   },
   "outputs": [],
   "source": [
    "np.save('messages_download_first_133',messages_donwload)\n",
    "np.save('acc_download_first_133',acc_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHsIEJ4NlEkV"
   },
   "outputs": [],
   "source": [
    "base_model.save(\"base_model_first_133.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz7zOSPklFBQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('base_model_first_133.h5')\n",
    "files.download('messages_download_first_133.npy')\n",
    "files.download('acc_download_first_133.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3Y7jHqjawm7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZvxBQtkZ0X9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADPZU2JdaFpO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "base_model_final = load_model('./finalv1.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-38QveSacRT"
   },
   "outputs": [],
   "source": [
    "average_model = five_average_model(input_shape = (n_freq, Tx), base_model = base_model_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Jm0D3EtaxZD"
   },
   "outputs": [],
   "source": [
    "X_test_predict = average_model.predict(X_test.tolist(),batch_size = 8,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqDk8p4AbQQR"
   },
   "outputs": [],
   "source": [
    "X_train_predict = average_model.predict(X_train.tolist(),batch_size = 4,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMh93B48a5HZ"
   },
   "outputs": [],
   "source": [
    "average_embeddings = average_emb(X_train_predict,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1kec_f5bBVs"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "bad_count = 0\n",
    "for i in range(len(X_test_predict)):\n",
    "  \n",
    "    best_result = -1\n",
    "    best_val = 0\n",
    "    for key, value in average_embeddings.items():\n",
    "        result = cosine_similarity(X_test_predict[i],value)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_val = key\n",
    "    if (best_val == Y_test[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        bad_count+=1\n",
    "print (count, bad_count)\n",
    "print (count/(count+bad_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ez70InqobhWn"
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train_predict)):\n",
    "    best_result = -1\n",
    "    best_val = 0\n",
    "    for key, value in average_embeddings.items():\n",
    "        result = cosine_similarity(X_train_predict[i],value)\n",
    "        if result > best_result:\n",
    "            best_result = result\n",
    "            best_val = key\n",
    "            \n",
    "    if (best_val == Y_train[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        bad_count+=1\n",
    "print (count, bad_count)\n",
    "print (count/(count+bad_count))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
